
service: vautomator-serverless
frameworkVersion: ">=1.2.0 <2.0.0"

provider:
  name: aws
  runtime: python3.6
  region: us-west-2
  # To use the bucket specified, we will need permissions
  iamRoleStatements:
   - Effect: "Allow"
     Action:
       - "s3:PutObject"
       - "s3:PutObjectAcl"
     Resource: 
      Fn::Join:
        - ""
        - - Fn::GetAtt:
            - S3BucketResults
            - Arn
          - "/*"
   - Effect: "Allow"
     Action:
      - "sqs:SendMessage"
     Resource:
      Fn::GetAtt: [ SQSQueue, Arn ]
   - Effect: Allow
     Action:
      - "sqs:SendMessage"
     Resource:
      - "Fn::GetAtt":
        - ReceiverDeadLetterQueue
        - Arn
  environment:
    HTTPOBS_API_URL: 'https://http-observatory.security.mozilla.org/api/v1'
    SQS_URL:
      Ref: SQSQueue

# Adding some packaging information here to clean up
# and to include the nmap static library
package:
  include:
    - bin/*
  exclude:
    - .venv/**
    - .git/**
    - __pycache__/**
    - node_modules/**

functions:
  onDemandPortScan:
    handler: handler.addPortScanToQueue
    events:
      - http:
          path: ondemand/portscan
          method: PUT
          cors: true
  cronPortScan:
    handler: handler.runScheduledPortScan
    # Giving 5 minutes to run the TCP scan before it times outs
    timeout: 300
    events:
      # Invoke Lambda function once a day
      # We can fine grain it with cron() later
      - schedule: 
          rate: rate(1 day)
          # Not enabling this by default
          enabled: false
  onDemandObservatoryScan:
    handler: handler.addObservatoryScanToQueue
    events:
      - http:
          path: ondemand
          method: PUT
          cors: true
  cronObservatoryScan:
    handler: handler.runScheduledObservatoryScan
    events:
      # Invoke Lambda function every 5 mins
      - schedule: rate(5 minutes)
      - s3:
        bucket: ${self:custom.cfg.s3BucketName}
        event: s3:ObjectCreated:*
  RunScanQueue:
    handler: handler.runScanFromQ
    # Giving this function a large timeout as well
    timeout: 300
    events:
      - sqs:
          arn:
            Fn::GetAtt: [ SQSQueue, Arn ]
          batchSize: 1

plugins:
  - serverless-python-requirements
resources:
  Resources:
    S3BucketResults:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:custom.cfg.s3BucketName}
    ReceiverDeadLetterQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: ${self:custom.cfg.vautomatorDLQ}
        MessageRetentionPeriod: 120
    SQSQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:custom.cfg.vautomatorQ}
        VisibilityTimeout: 300
        MessageRetentionPeriod: 60
        RedrivePolicy:
          deadLetterTargetArn:
            "Fn::GetAtt":
              - ReceiverDeadLetterQueue
              - Arn
          maxReceiveCount: 1
custom:
  cfg:
    s3BucketName: "vautomator-results"
    vautomatorQ: "vautomator-SQS"
    vautomatorDLQ: "vautomator-DLQ"
  pythonRequirements:
    # dockerFile: Dockerfile
    dockerizePip: true
